{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Variational Autoencoder (VAE) for KDD CUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Method\n",
    "\n",
    "1. Prepare data & Partition the data into 80-20 train-test split\n",
    "2. Define Models & Parameters\n",
    "3. Perform Cross-Validation on the training data\n",
    "3. (Re)Train the models on the training data, either vanilla model or with best hyperparameters\n",
    "4. Evaluate the (re)trained models on the test data\n",
    "5. Final evaluation of the models\n",
    "\n",
    "![The Method](method.png)\n",
    "\n",
    "Source: [scikit-learn.org - Cross-validation](https://scikit-learn.org/1.5/modules/cross_validation.html#cross-validation-and-model-selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame as df\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as imblearn_pipeline # Use imblearn's pipeline\n",
    "from sklearn.metrics import make_scorer, classification_report, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.base import  BaseEstimator,ClassifierMixin\n",
    "\n",
    "# Import ML libraries\n",
    "import keras\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "import keras_tuner\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "import tensorflow as tf\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "CROSS_VAL_SPLITS = 5\n",
    "CKP_PREFIX = 'KDD_'\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# pandas options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare data & Partition the data into 80-20 train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "path_to_data = '../data/kddcup/kddcup_data_corrected.csv'\n",
    "\n",
    "# col names from https://kdd.ics.uci.edu/databases/kddcup99/kddcup.names\n",
    "col_names = [\n",
    "    \"duration\",\n",
    "    \"protocol_type\",\n",
    "    \"service\",\n",
    "    \"flag\",\n",
    "    \"src_bytes\",\n",
    "    \"dst_bytes\",\n",
    "    \"land\",\n",
    "    \"wrong_fragment\",\n",
    "    \"urgent\",\n",
    "    \"hot\",\n",
    "    \"num_failed_logins\",\n",
    "    \"logged_in\",\n",
    "    \"num_compromised\",\n",
    "    \"root_shell\",\n",
    "    \"su_attempted\",\n",
    "    \"num_root\",\n",
    "    \"num_file_creations\",\n",
    "    \"num_shells\",\n",
    "    \"num_access_files\",\n",
    "    \"num_outbound_cmds\",\n",
    "    \"is_host_login\",\n",
    "    \"is_guest_login\",\n",
    "    \"count\",\n",
    "    \"srv_count\",\n",
    "    \"serror_rate\",\n",
    "    \"srv_serror_rate\",\n",
    "    \"rerror_rate\",\n",
    "    \"srv_rerror_rate\",\n",
    "    \"same_srv_rate\",\n",
    "    \"diff_srv_rate\",\n",
    "    \"srv_diff_host_rate\",\n",
    "    \"dst_host_count\",\n",
    "    \"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\",\n",
    "    \"dst_host_diff_srv_rate\",\n",
    "    \"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\",\n",
    "    \"dst_host_serror_rate\",\n",
    "    \"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\",\n",
    "    \"dst_host_srv_rerror_rate\",\n",
    "    \"label\",\n",
    "]\n",
    "# from https://kdd.ics.uci.edu/databases/kddcup99/kddcup.names\n",
    "categorical_cols = [\n",
    "    \"protocol_type\",\n",
    "    \"service\",\n",
    "    \"flag\",\n",
    "    \"land\",\n",
    "    \"logged_in\",\n",
    "    \"is_host_login\",\n",
    "    \"is_guest_login\",\n",
    "]\n",
    "\n",
    "# Read data (10 % subset of the original dataset)\n",
    "data = pd.read_csv(path_to_data, names=col_names, header=None)\n",
    "\n",
    "# Label encoding\n",
    "data[\"label\"] = data[\"label\"].apply(lambda x: \"attack\" if x != \"normal.\" else 'normal')\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(data[\"label\"])\n",
    "X = data.drop(\"label\", axis=1)\n",
    "\n",
    "# Extract label mapping\n",
    "label_mapping = {label: index for index, label in enumerate(le.classes_)}\n",
    "normal_label = label_mapping['normal']\n",
    "attack_label = label_mapping['attack']\n",
    "print(\"Label Mapping:\", label_mapping)\n",
    "print(\"Normal Label:\", normal_label)\n",
    "print(\"Attack Label:\", attack_label)\n",
    "\n",
    "# Visualize class imbalance\n",
    "plt.bar([\"Attack\", \"Normal\"], data[\"label\"].value_counts())\n",
    "total = len(data)\n",
    "fraud_percentage = (data[\"label\"].value_counts()[1] / total) * 100\n",
    "normal_percentage = (data[\"label\"].value_counts()[0] / total) * 100\n",
    "\n",
    "plt.text(0, data[\"label\"].value_counts()[0] / 2, f'{normal_percentage:.2f}%', ha='center', color='black')\n",
    "plt.text(1, data[\"label\"].value_counts()[1] / 2, f'{fraud_percentage:.2f}%', ha='center', color='black')\n",
    "plt.title(\"Class Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# create training and test partitions with 80-20 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "# Fit OneHotEncoder to learn categories from the training data. This is necessary to ensure that the same categories are used for random splits during cross-validation\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False, dtype=\"int8\")\n",
    "one_hot_encoder.fit(X_train[categorical_cols])\n",
    "learned_categories = one_hot_encoder.categories_\n",
    "\n",
    "# Define numerical columns & indices\n",
    "num_cols = data.drop(categorical_cols, axis=1)\n",
    "num_indices = [data.columns.get_loc(col) for col in num_cols.drop('label', axis=1).columns]\n",
    "cat_indices = [data.columns.get_loc(col) for col in categorical_cols]\n",
    "\n",
    "# Set class weights\n",
    "class_weight = {label[0]: 1.0 / count for label, count in df(y_train).value_counts().items()}\n",
    "print(\"Weights: \", class_weight)\n",
    "\n",
    "# Define preprocessing transformers and pipeline\n",
    "\n",
    "# Numerical transformer. Scales numerical data using StandardScaler\n",
    "numeric_transformer = imblearn_pipeline(\n",
    "    steps=[(\"scaler\", StandardScaler())], verbose=True\n",
    ")\n",
    "numeric_transformer.set_output(transform=\"pandas\")\n",
    "\n",
    "# Categorical transformer. Encodes categorical data using OneHotEncoder\n",
    "categorical_transformer = imblearn_pipeline(\n",
    "    steps=[\n",
    "        (\n",
    "            \"onehot\",\n",
    "            OneHotEncoder(\n",
    "                # handle_unknown=\"ignore\",\n",
    "                sparse_output=False,\n",
    "                categories=learned_categories,\n",
    "                dtype=\"int8\",\n",
    "            ),\n",
    "        )\n",
    "    ],\n",
    "    verbose=True,\n",
    ")\n",
    "categorical_transformer.set_output(transform=\"pandas\")\n",
    "\n",
    "# Define preprocessor with ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_indices),\n",
    "        (\"cat\", categorical_transformer, cat_indices),\n",
    "    ],\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Check preprocessing pipeline and get target shape\n",
    "transformed_sample = preprocessor.fit_transform(X_train)\n",
    "print(f\"Transformed X_train shape: {transformed_sample.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define models and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE Keras Custom Model Class \n",
    "\n",
    "# Code based on Bank et al (2021)\n",
    "@keras.saving.register_keras_serializable()\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, kl_beta=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.decoder = decoder\n",
    "        self.encoder = encoder\n",
    "        self.loss_tracker =  keras.metrics.Mean(name=\"loss\")\n",
    "        self.seed_generator = keras.random.SeedGenerator(RANDOM_STATE)\n",
    "        self.kl_beta = kl_beta # weight to put more or less balance on reconstruction loss\n",
    "        \n",
    "    def compile(self, optimizer):\n",
    "        super().compile()\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    # @tf.function\n",
    "    def reconstruction_loss(self, x, f_z):\n",
    "        #reconstruction_loss = keras.losses.binary_crossentropy(x, f_z)\n",
    "        mse = keras.losses.MeanSquaredError()\n",
    "        reconstruction_loss = mse(x, f_z)\n",
    "\n",
    "        return reconstruction_loss\n",
    "    \n",
    "    # @tf.function\n",
    "    def loss_fn(self, x, f_z, z_mean, z_log_var):\n",
    "        # Ensure reduction across the batch dimension\n",
    "        reconstruction_loss = tf.reduce_mean(self.reconstruction_loss(x, f_z))\n",
    "        \n",
    "        # KL divergence calculation\n",
    "        kl_loss = -0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var + 1e-8), axis=1)\n",
    "        kl_loss = tf.reduce_mean(kl_loss)\n",
    "        \n",
    "        # total loss calculation\n",
    "        total_loss = reconstruction_loss + (self.kl_beta * kl_loss)\n",
    "        return total_loss, reconstruction_loss, kl_loss\n",
    "    \n",
    "    @staticmethod # Define the encoder model. See Bank et al (2021, p 8)\n",
    "    def get_encoder(data_dim, latent_dim, hidden_dims, activation=keras.layers.ReLU(), dropout_rate=0): \n",
    "        \"\"\" Encoder Net\n",
    "        \n",
    "        Takes input x and maps it into a mean and covariance that determine the approximate posterior distribution of the latent space.\n",
    "\n",
    "        Args:\n",
    "            data_dim (_type_): Input dimension. Usually equal to n features.\n",
    "            latent_dim (_type_): Size of the bottleneck.\n",
    "            hidden_dims: List of hidden dims, e.g., [20, 10, 8]\n",
    "            activtion: Activtion function for hidden dims\n",
    "\n",
    "        Returns:\n",
    "            z_mean: Mean\n",
    "            z_log_var: Covariance\n",
    "        \"\"\"\n",
    "        # Input Layer\n",
    "        inputs = layers.Input(shape=(data_dim,))\n",
    "        \n",
    "        # Hidden Layer for compression\n",
    "        x = inputs\n",
    "        for dim in hidden_dims:\n",
    "            x = layers.Dense(dim)(x)\n",
    "            x = activation(x)\n",
    "            if dropout_rate != 0:\n",
    "                x = layers.Dropout(dropout_rate)(x)\n",
    "              \n",
    "        # calculate mean g(x) and covariance h(x) of the distribution\n",
    "        z_mean = layers.Dense(latent_dim)(x)\n",
    "        z_log_var = layers.Dense(latent_dim)(x)\n",
    "\n",
    "        return keras.Model(inputs, [z_mean, z_log_var], name='encoder')\n",
    "    \n",
    "    @staticmethod # Define the decoder model f(z)\n",
    "    def get_decoder(latent_dim, output_dim, hidden_dims, activation=keras.layers.ReLU(), dropout_rate=0):\n",
    "        # take sample from latent space as input\n",
    "        latent_inputs = layers.Input(shape=(latent_dim,))\n",
    "        \n",
    "        # Hidden Layer for reconstruction\n",
    "        x = latent_inputs\n",
    "        for dim in reversed(hidden_dims):\n",
    "            x = layers.Dense(dim)(x)\n",
    "            x = activation(x)\n",
    "            if dropout_rate != 0:\n",
    "                x = layers.Dropout(dropout_rate)(x)\n",
    "        \n",
    "        # output reconstruction\n",
    "        outputs = layers.Dense(output_dim, activation='sigmoid')(x)\n",
    "        return keras.Model(latent_inputs, outputs, name='decoder')\n",
    "    \n",
    "    def sampling(self, args): # Define the sampling function. See Bank et al (2021, p 8)\n",
    "        \"\"\" Sample from latent space by applying 'Reparameterization'\n",
    "\n",
    "        Args:\n",
    "            args (list): List of mean and covariance (z_mean, z_log_var)\n",
    "\n",
    "        Returns:\n",
    "            z: Sample from the latent space\n",
    "        \"\"\"\n",
    "        # unpack mean g(x) and covariance h(x)\n",
    "        z_mean, z_log_var = args\n",
    "        \n",
    "        # epsilon is a normal distribution: N(0, I)\n",
    "        epsilon = keras.random.normal(shape=tf.shape(z_mean), seed=self.seed_generator)\n",
    "        \n",
    "        # Sampling via reparametrisation trick: z = h(x)  * espilon + g(x)\n",
    "        return epsilon * tf.exp(z_log_var * .5) + z_mean\n",
    "\n",
    "    # @tf.function\n",
    "    def train_step(self, data):\n",
    "        # data = random minibatch of M datapoints\n",
    "        x = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # encode input to mean g(x) and covariance h(x)\n",
    "            z_mean, z_log_var = self.encoder(x)\n",
    "            \n",
    "            # sample random z from latent space\n",
    "            z = self.sampling((z_mean, z_log_var))\n",
    "            \n",
    "            # decode z via f(z)\n",
    "            f_z = self.decoder(z)\n",
    "\n",
    "            # calculate reconstruction loss\n",
    "            loss, reconstruction_loss, kl_loss = self.loss_fn(x, f_z, z_mean, z_log_var)\n",
    "            \n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)         \n",
    "        \n",
    "        return {\n",
    "            \"loss\": self.loss_tracker.result(),\n",
    "            \"reconstruction_loss\": reconstruction_loss,\n",
    "            \"kl_loss\": kl_loss\n",
    "        }\n",
    "        \n",
    "    def test_step(self, data):\n",
    "        x = data\n",
    "        z_mean, z_log_var = self.encoder(x)\n",
    "        \n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        f_z = self.decoder(z)\n",
    "        \n",
    "        _, reconstruction_loss, _ = self.loss_fn(x, f_z, z_mean, z_log_var)\n",
    "        return {\n",
    "            \"reconstruction_loss\": reconstruction_loss,\n",
    "        }\n",
    "\n",
    "    # def call(self, data):\n",
    "    #     x = data\n",
    "    #     z_mean, z_log_var = self.encoder(x)\n",
    "        \n",
    "    #     z = self.sampling((z_mean, z_log_var))\n",
    "    #     f_z = self.decoder(z)\n",
    "            \n",
    "    #     return self.reconstruction_loss(x, f_z)\n",
    "\n",
    "    def call(self, data):\n",
    "        x = data\n",
    "        z_mean, z_log_var = self.encoder(x)\n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        f_z = self.decoder(z)\n",
    "        return f_z  # Return reconstructed outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, data_dim, latent_dim=80, hidden_dims=[128, 128, 64], \n",
    "                 kl_beta=0.5, dropout_rate=0.0, batch_size=128, epochs=100, learning_rate=1e-6, cb_early_stopping_patience=5, normal_label=1):\n",
    "        self.data_dim = data_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.kl_beta = kl_beta\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.vae = None\n",
    "        self.activation = keras.layers.ReLU()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.cb_early_stopping_patience = cb_early_stopping_patience\n",
    "        self.normal_label = normal_label\n",
    "\n",
    "        print(\"type of learning rate: \", type(self.learning_rate))\n",
    "    def _build_model(self):\n",
    "\n",
    "        enc = VAE.get_encoder(self.data_dim, self.latent_dim, self.hidden_dims, \n",
    "                              self.activation, self.dropout_rate)\n",
    "        dec = VAE.get_decoder(self.latent_dim, self.data_dim, self.hidden_dims, \n",
    "                              self.activation, self.dropout_rate)\n",
    "        vae = VAE(enc, dec, self.kl_beta)\n",
    "\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        vae.compile(optimizer=optimizer)\n",
    "        return vae\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X_train = X[y == self.normal_label]  # Only train on normal samples\n",
    "        X_train = np.array(X_train, dtype=np.float32)\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(X_train).batch(self.batch_size)\n",
    "        self.vae = self._build_model()\n",
    "        self.vae.fit(dataset, epochs=self.epochs, verbose=1, callbacks=[\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                patience=self.cb_early_stopping_patience, monitor='reconstruction_loss', mode='min'\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        # Compute reconstruction losses on the training data\n",
    "        reconstructed_X = self.vae.predict(X_train)\n",
    "        self.training_losses = np.mean(np.square(X_train - reconstructed_X), axis=1)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        reconstructed_X = self.vae.predict(X)\n",
    "        \n",
    "        # Compute per-sample reconstruction losses\n",
    "        losses = np.mean(np.square(X - reconstructed_X), axis=1)\n",
    "        \n",
    "        # Determine the threshold based on training losses\n",
    "        threshold = np.percentile(self.training_losses, 95)  # Adjust percentile if needed\n",
    "        predictions = (losses > threshold).astype(int)  # 1 indicates anomaly (attack)\n",
    "        \n",
    "        print(f\"Threshold used: {threshold}\")\n",
    "        print(f\"Predictions unique values: {np.unique(predictions, return_counts=True)}\")\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_estimator = VAEClassifier(data_dim=transformed_sample.shape[1], normal_label=normal_label)\n",
    "\n",
    "# Define pipeline\n",
    "vae_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', vae_estimator)\n",
    "])\n",
    "\n",
    "# Define scoring metrics\n",
    "scoring = {\n",
    "    'f1': make_scorer(f1_score, zero_division=0),\n",
    "    'precision': make_scorer(precision_score, zero_division=0),\n",
    "    'recall': make_scorer(recall_score, zero_division=0)\n",
    "}\n",
    "\n",
    "# test the model\n",
    "vae_pipeline.fit(X_train[y_train == normal_label], y_train[y_train == normal_label])\n",
    "y_pred = vae_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = vae_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transformed = preprocessor.transform(X_test[y_test == normal_label])\n",
    "print(\"Test transformed shape: \", test_transformed.shape)\n",
    "\n",
    "# Compute reconstruction losses manually\n",
    "reconstructed_X = vae_pipeline.named_steps['classifier'].vae.predict(test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate f1\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "# Ensure the data is in the correct format\n",
    "X_train_transformed = np.array(X_train_transformed, dtype=np.float32)\n",
    "X_test_transformed = np.array(X_test_transformed, dtype=np.float32)\n",
    "\n",
    "# Initialize the VAEClassifier\n",
    "vae_classifier = VAEClassifier(\n",
    "    data_dim=X_train_transformed.shape[1],  # Adjust based on your input shape\n",
    "    latent_dim=80,  # Modify latent_dim based on your requirement\n",
    "    hidden_dims=[128, 128, 64],  # Adjust hidden layer dimensions if necessary\n",
    "    kl_beta=0.5,\n",
    "    dropout_rate=0.0,\n",
    "    batch_size=128,\n",
    "    epochs=10,  # Set to a lower value for quick testing; adjust as needed\n",
    "    learning_rate=1e-3,  # Adjust as per your need\n",
    "    cb_early_stopping_patience=5,\n",
    "    normal_label=normal_label  # Make sure 'normal_label' is correctly defined\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "print(\"Starting model training...\")\n",
    "vae_classifier.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "print(\"Making predictions...\")\n",
    "predictions = vae_classifier.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate F1 Score\n",
    "print(\"Evaluating F1 score...\")\n",
    "f1 = f1_score(y_test, predictions, average='binary')  # Use 'binary' or 'macro' depending on your target setup\n",
    "print(f\"F1 Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perform cross-validation\n",
    "skf = StratifiedKFold(n_splits=2)\n",
    "results = cross_validate(vae_pipeline, X_train[y_train == normal_label], y_train[y_train == normal_label], cv=skf, scoring=scoring, return_train_score=False)\n",
    "\n",
    "# Print results\n",
    "for metric in scoring.keys():\n",
    "    mean_score = np.mean(results[f'test_{metric}'])\n",
    "    std_score = np.std(results[f'test_{metric}'])\n",
    "    print(f\"{metric.capitalize()} - Mean: {mean_score:.4f}, Std: {std_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of learning rate:  <class 'float'>\n",
      "[CV 1 ...]\n",
      "[Pipeline] ............ (step 1 of 1) Processing scaler, total=   0.0s\n",
      "[ColumnTransformer] ........... (1 of 2) Processing num, total=   0.0s\n",
      "[Pipeline] ............ (step 1 of 1) Processing onehot, total=   0.0s\n",
      "[ColumnTransformer] ........... (2 of 2) Processing cat, total=   0.0s\n",
      "Epoch 1/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - kl_loss: 806.7906 - loss: 305.2220 - reconstruction_loss: 0.4954\n",
      "Epoch 2/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 765.0170 - loss: 289.5061 - reconstruction_loss: 0.4938\n",
      "Epoch 3/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 719.7486 - loss: 272.4652 - reconstruction_loss: 0.4920\n",
      "Epoch 4/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 676.7652 - loss: 256.2779 - reconstruction_loss: 0.4902\n",
      "Epoch 5/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 636.9020 - loss: 241.2633 - reconstruction_loss: 0.4887\n",
      "Epoch 6/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 600.1505 - loss: 227.4200 - reconstruction_loss: 0.4870\n",
      "Epoch 7/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 566.2994 - loss: 214.6693 - reconstruction_loss: 0.4852\n",
      "Epoch 8/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 535.0648 - loss: 202.9039 - reconstruction_loss: 0.4836\n",
      "Epoch 9/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 506.1899 - loss: 192.0276 - reconstruction_loss: 0.4819\n",
      "Epoch 10/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 479.4413 - loss: 181.9524 - reconstruction_loss: 0.4802\n",
      "Epoch 11/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 454.5995 - loss: 172.5949 - reconstruction_loss: 0.4784\n",
      "Epoch 12/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 431.4769 - loss: 163.8851 - reconstruction_loss: 0.4767\n",
      "Epoch 13/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 409.9044 - loss: 155.7590 - reconstruction_loss: 0.4749\n",
      "Epoch 14/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 389.7373 - loss: 148.1621 - reconstruction_loss: 0.4732\n",
      "Epoch 15/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 370.8485 - loss: 141.0462 - reconstruction_loss: 0.4714\n",
      "Epoch 16/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 353.1230 - loss: 134.3684 - reconstruction_loss: 0.4696\n",
      "Epoch 17/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 336.4619 - loss: 128.0914 - reconstruction_loss: 0.4677\n",
      "Epoch 18/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 320.3212 - loss: 122.0327 - reconstruction_loss: 0.4659\n",
      "Epoch 19/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 305.1029 - loss: 116.3198 - reconstruction_loss: 0.4640\n",
      "Epoch 20/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 290.9075 - loss: 110.9827 - reconstruction_loss: 0.4620\n",
      "Epoch 21/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 277.5713 - loss: 105.9645 - reconstruction_loss: 0.4601\n",
      "Epoch 22/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 265.0033 - loss: 101.2329 - reconstruction_loss: 0.4581\n",
      "Epoch 23/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 253.2273 - loss: 96.7939 - reconstruction_loss: 0.4561\n",
      "Epoch 24/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 242.4521 - loss: 92.7345 - reconstruction_loss: 0.4541\n",
      "Epoch 25/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 232.2278 - loss: 88.8749 - reconstruction_loss: 0.4519\n",
      "Epoch 26/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 222.4579 - loss: 85.1875 - reconstruction_loss: 0.4499\n",
      "Epoch 27/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 213.1978 - loss: 81.6927 - reconstruction_loss: 0.4477\n",
      "Epoch 28/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 204.3901 - loss: 78.3734 - reconstruction_loss: 0.4457\n",
      "Epoch 29/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 196.0603 - loss: 75.2325 - reconstruction_loss: 0.4435\n",
      "Epoch 30/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 188.0729 - loss: 72.2175 - reconstruction_loss: 0.4415\n",
      "Epoch 31/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 180.4644 - loss: 69.3455 - reconstruction_loss: 0.4393\n",
      "Epoch 32/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 173.2135 - loss: 66.6083 - reconstruction_loss: 0.4372\n",
      "Epoch 33/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 166.2624 - loss: 63.9861 - reconstruction_loss: 0.4351\n",
      "Epoch 34/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 159.3632 - loss: 61.3943 - reconstruction_loss: 0.4329\n",
      "Epoch 35/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 152.9130 - loss: 58.9643 - reconstruction_loss: 0.4307\n",
      "Epoch 36/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 146.8181 - loss: 56.6646 - reconstruction_loss: 0.4285\n",
      "Epoch 37/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 141.0318 - loss: 54.4792 - reconstruction_loss: 0.4262\n",
      "Epoch 38/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 135.5271 - loss: 52.3983 - reconstruction_loss: 0.4240\n",
      "Epoch 39/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 130.2743 - loss: 50.4101 - reconstruction_loss: 0.4216\n",
      "Epoch 40/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 125.2691 - loss: 48.5156 - reconstruction_loss: 0.4190\n",
      "Epoch 41/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 120.4880 - loss: 46.7051 - reconstruction_loss: 0.4166\n",
      "Epoch 42/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 115.9180 - loss: 44.9738 - reconstruction_loss: 0.4142\n",
      "Epoch 43/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 111.5470 - loss: 43.3167 - reconstruction_loss: 0.4117\n",
      "Epoch 44/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 107.3651 - loss: 41.7311 - reconstruction_loss: 0.4093\n",
      "Epoch 45/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 103.3617 - loss: 40.2125 - reconstruction_loss: 0.4069\n",
      "Epoch 46/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 99.5286 - loss: 38.7578 - reconstruction_loss: 0.4044 \n",
      "Epoch 47/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 95.8569 - loss: 37.3639 - reconstruction_loss: 0.4021 \n",
      "Epoch 48/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - kl_loss: 92.3391 - loss: 36.0275 - reconstruction_loss: 0.3994\n",
      "Epoch 49/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 88.9674 - loss: 34.7462 - reconstruction_loss: 0.3967\n",
      "Epoch 50/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 85.7353 - loss: 33.5171 - reconstruction_loss: 0.3942\n",
      "Epoch 51/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 82.6359 - loss: 32.3379 - reconstruction_loss: 0.3914\n",
      "Epoch 52/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 79.6628 - loss: 31.2062 - reconstruction_loss: 0.3887\n",
      "Epoch 53/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 76.8107 - loss: 30.1200 - reconstruction_loss: 0.3863\n",
      "Epoch 54/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 74.0736 - loss: 29.0767 - reconstruction_loss: 0.3834\n",
      "Epoch 55/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 71.4438 - loss: 28.0742 - reconstruction_loss: 0.3808\n",
      "Epoch 56/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 68.9175 - loss: 27.1104 - reconstruction_loss: 0.3781\n",
      "Epoch 57/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - kl_loss: 66.4925 - loss: 26.1848 - reconstruction_loss: 0.3756\n",
      "Epoch 58/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 64.1661 - loss: 25.2962 - reconstruction_loss: 0.3730\n",
      "Epoch 59/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 61.9315 - loss: 24.4422 - reconstruction_loss: 0.3704\n",
      "Epoch 60/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 59.7841 - loss: 23.6206 - reconstruction_loss: 0.3675\n",
      "Epoch 61/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 57.7201 - loss: 22.8306 - reconstruction_loss: 0.3651\n",
      "Epoch 62/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - kl_loss: 55.5591 - loss: 22.0130 - reconstruction_loss: 0.3625\n",
      "Epoch 63/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 53.5858 - loss: 21.2603 - reconstruction_loss: 0.3598\n",
      "Epoch 64/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - kl_loss: 51.7136 - loss: 20.5442 - reconstruction_loss: 0.3569\n",
      "Epoch 65/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 49.9249 - loss: 19.8590 - reconstruction_loss: 0.3544\n",
      "Epoch 66/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 48.2657 - loss: 19.2191 - reconstruction_loss: 0.3514\n",
      "Epoch 67/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 46.6640 - loss: 18.6030 - reconstruction_loss: 0.3489\n",
      "Epoch 68/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 45.1511 - loss: 18.0178 - reconstruction_loss: 0.3463\n",
      "Epoch 69/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 43.6772 - loss: 17.4491 - reconstruction_loss: 0.3437\n",
      "Epoch 70/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 42.2551 - loss: 16.8988 - reconstruction_loss: 0.3409\n",
      "Epoch 71/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 40.8905 - loss: 16.3716 - reconstruction_loss: 0.3385\n",
      "Epoch 72/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 39.5714 - loss: 15.8602 - reconstruction_loss: 0.3357\n",
      "Epoch 73/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 38.2960 - loss: 15.3664 - reconstruction_loss: 0.3331\n",
      "Epoch 74/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 37.0699 - loss: 14.8906 - reconstruction_loss: 0.3309\n",
      "Epoch 75/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 35.8799 - loss: 14.4291 - reconstruction_loss: 0.3279\n",
      "Epoch 76/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 34.7441 - loss: 13.9875 - reconstruction_loss: 0.3256\n",
      "Epoch 77/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 33.6336 - loss: 13.5570 - reconstruction_loss: 0.3231\n",
      "Epoch 78/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - kl_loss: 32.5723 - loss: 13.1449 - reconstruction_loss: 0.3208\n",
      "Epoch 79/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 31.5423 - loss: 12.7434 - reconstruction_loss: 0.3181\n",
      "Epoch 80/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 30.5434 - loss: 12.3552 - reconstruction_loss: 0.3157\n",
      "Epoch 81/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 29.5890 - loss: 11.9838 - reconstruction_loss: 0.3134\n",
      "Epoch 82/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - kl_loss: 28.6586 - loss: 11.6211 - reconstruction_loss: 0.3113\n",
      "Epoch 83/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 27.7584 - loss: 11.2706 - reconstruction_loss: 0.3089\n",
      "Epoch 84/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 26.8959 - loss: 10.9342 - reconstruction_loss: 0.3067\n",
      "Epoch 85/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 26.0579 - loss: 10.6069 - reconstruction_loss: 0.3044\n",
      "Epoch 86/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 25.2404 - loss: 10.2885 - reconstruction_loss: 0.3023\n",
      "Epoch 87/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 24.4365 - loss: 9.9761 - reconstruction_loss: 0.3004\n",
      "Epoch 88/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 23.6660 - loss: 9.6753 - reconstruction_loss: 0.2981\n",
      "Epoch 89/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 22.9230 - loss: 9.3858 - reconstruction_loss: 0.2963\n",
      "Epoch 90/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 22.2115 - loss: 9.1079 - reconstruction_loss: 0.2944\n",
      "Epoch 91/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 21.5182 - loss: 8.8365 - reconstruction_loss: 0.2926\n",
      "Epoch 92/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - kl_loss: 20.8485 - loss: 8.5745 - reconstruction_loss: 0.2907\n",
      "Epoch 93/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 20.2069 - loss: 8.3236 - reconstruction_loss: 0.2891\n",
      "Epoch 94/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 19.5897 - loss: 8.0813 - reconstruction_loss: 0.2873\n",
      "Epoch 95/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 18.9892 - loss: 7.8464 - reconstruction_loss: 0.2857\n",
      "Epoch 96/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 18.4099 - loss: 7.6194 - reconstruction_loss: 0.2841\n",
      "Epoch 97/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 17.8537 - loss: 7.4009 - reconstruction_loss: 0.2823\n",
      "Epoch 98/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 17.3147 - loss: 7.1890 - reconstruction_loss: 0.2809\n",
      "Epoch 99/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 16.7914 - loss: 6.9839 - reconstruction_loss: 0.2796\n",
      "Epoch 100/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - kl_loss: 16.2858 - loss: 6.7855 - reconstruction_loss: 0.2782\n",
      "\u001b[1m758/758\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498us/step\n",
      "\u001b[1m758/758\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 460us/step\n",
      "Threshold used: 0.39954145550727854\n",
      "Predictions unique values: (array([0, 1]), array([23069,  1168]))\n",
      "type of y_pred: <class 'numpy.ndarray'>\n",
      "[CV 2 ...]\n",
      "[Pipeline] ............ (step 1 of 1) Processing scaler, total=   0.0s\n",
      "[ColumnTransformer] ........... (1 of 2) Processing num, total=   0.0s\n",
      "[Pipeline] ............ (step 1 of 1) Processing onehot, total=   0.0s\n",
      "[ColumnTransformer] ........... (2 of 2) Processing cat, total=   0.0s\n",
      "Epoch 1/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - kl_loss: 20391.1758 - loss: 1226.7631 - reconstruction_loss: 0.5071\n",
      "Epoch 2/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 18647.3242 - loss: 1125.7882 - reconstruction_loss: 0.5055\n",
      "Epoch 3/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 17100.3613 - loss: 1036.3674 - reconstruction_loss: 0.5037\n",
      "Epoch 4/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 15729.7139 - loss: 956.6968 - reconstruction_loss: 0.5021\n",
      "Epoch 5/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 14511.3213 - loss: 885.6051 - reconstruction_loss: 0.5004\n",
      "Epoch 6/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - kl_loss: 13421.0117 - loss: 821.7574 - reconstruction_loss: 0.4987\n",
      "Epoch 7/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - kl_loss: 12438.9971 - loss: 764.1019 - reconstruction_loss: 0.4970\n",
      "Epoch 8/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 11536.8916 - loss: 711.0709 - reconstruction_loss: 0.4955\n",
      "Epoch 9/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 10724.8418 - loss: 663.2076 - reconstruction_loss: 0.4938\n",
      "Epoch 10/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 9988.2412 - loss: 619.6603 - reconstruction_loss: 0.4921 \n",
      "Epoch 11/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 9317.3076 - loss: 579.8879 - reconstruction_loss: 0.4901\n",
      "Epoch 12/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - kl_loss: 8702.7490 - loss: 543.3658 - reconstruction_loss: 0.4885\n",
      "Epoch 13/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 8138.9287 - loss: 509.7739 - reconstruction_loss: 0.4869\n",
      "Epoch 14/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 7620.0229 - loss: 478.7820 - reconstruction_loss: 0.4852\n",
      "Epoch 15/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - kl_loss: 7140.9092 - loss: 450.0987 - reconstruction_loss: 0.4834\n",
      "Epoch 16/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - kl_loss: 6697.6187 - loss: 423.5004 - reconstruction_loss: 0.4817\n",
      "Epoch 17/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 6286.8296 - loss: 398.7937 - reconstruction_loss: 0.4802\n",
      "Epoch 18/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 5905.5723 - loss: 375.8093 - reconstruction_loss: 0.4782\n",
      "Epoch 19/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 5550.6958 - loss: 354.3670 - reconstruction_loss: 0.4766\n",
      "Epoch 20/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - kl_loss: 5209.1025 - loss: 333.7404 - reconstruction_loss: 0.4748\n",
      "Epoch 21/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 4891.9854 - loss: 314.5384 - reconstruction_loss: 0.4732\n",
      "Epoch 22/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 4599.8130 - loss: 296.7862 - reconstruction_loss: 0.4712\n",
      "Epoch 23/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 4328.3604 - loss: 280.1737 - reconstruction_loss: 0.4695\n",
      "Epoch 24/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - kl_loss: 4075.4695 - loss: 264.6511 - reconstruction_loss: 0.4677\n",
      "Epoch 25/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 3839.3276 - loss: 250.1399 - reconstruction_loss: 0.4658\n",
      "Epoch 26/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 3618.4578 - loss: 236.5426 - reconstruction_loss: 0.4638\n",
      "Epoch 27/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 3411.5339 - loss: 223.7788 - reconstruction_loss: 0.4620\n",
      "Epoch 28/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - kl_loss: 3217.5327 - loss: 211.7862 - reconstruction_loss: 0.4601\n",
      "Epoch 29/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 3035.4329 - loss: 200.5047 - reconstruction_loss: 0.4578\n",
      "Epoch 30/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 2864.4294 - loss: 189.8869 - reconstruction_loss: 0.4561\n",
      "Epoch 31/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 2703.6990 - loss: 179.8838 - reconstruction_loss: 0.4536\n",
      "Epoch 32/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 2552.5798 - loss: 170.4572 - reconstruction_loss: 0.4514\n",
      "Epoch 33/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 2410.4026 - loss: 161.5675 - reconstruction_loss: 0.4495\n",
      "Epoch 34/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 2276.6050 - loss: 153.1818 - reconstruction_loss: 0.4473\n",
      "Epoch 35/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - kl_loss: 2150.6360 - loss: 145.2679 - reconstruction_loss: 0.4456\n",
      "Epoch 36/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 2031.9982 - loss: 137.7957 - reconstruction_loss: 0.4435\n",
      "Epoch 37/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 1920.2487 - loss: 130.7391 - reconstruction_loss: 0.4415\n",
      "Epoch 38/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 1814.9508 - loss: 124.0728 - reconstruction_loss: 0.4397\n",
      "Epoch 39/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 1715.7162 - loss: 117.7734 - reconstruction_loss: 0.4374\n",
      "Epoch 40/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 1622.1611 - loss: 111.8191 - reconstruction_loss: 0.4356\n",
      "Epoch 41/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 1550.4318 - loss: 107.0985 - reconstruction_loss: 0.4334\n",
      "Epoch 42/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 1454.6141 - loss: 101.0995 - reconstruction_loss: 0.4315\n",
      "Epoch 43/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 1370.0527 - loss: 95.7445 - reconstruction_loss: 0.4292\n",
      "Epoch 44/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 1296.6357 - loss: 91.0110 - reconstruction_loss: 0.4269\n",
      "Epoch 45/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - kl_loss: 1227.3019 - loss: 86.5296 - reconstruction_loss: 0.4249\n",
      "Epoch 46/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 1161.8245 - loss: 82.2850 - reconstruction_loss: 0.4228\n",
      "Epoch 47/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 1099.9949 - loss: 78.2661 - reconstruction_loss: 0.4208\n",
      "Epoch 48/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 1041.6041 - loss: 74.4595 - reconstruction_loss: 0.4187\n",
      "Epoch 49/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 986.4610 - loss: 70.8544 - reconstruction_loss: 0.4164 \n",
      "Epoch 50/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 934.3910 - loss: 67.4397 - reconstruction_loss: 0.4139\n",
      "Epoch 51/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 885.1990 - loss: 64.2041 - reconstruction_loss: 0.4118\n",
      "Epoch 52/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - kl_loss: 838.7460 - loss: 61.1391 - reconstruction_loss: 0.4094\n",
      "Epoch 53/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 794.8707 - loss: 58.2345 - reconstruction_loss: 0.4072\n",
      "Epoch 54/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - kl_loss: 753.4175 - loss: 55.4811 - reconstruction_loss: 0.4046\n",
      "Epoch 55/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 714.2634 - loss: 52.8717 - reconstruction_loss: 0.4021\n",
      "Epoch 56/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - kl_loss: 677.2697 - loss: 50.3984 - reconstruction_loss: 0.3999\n",
      "Epoch 57/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 642.3205 - loss: 48.0529 - reconstruction_loss: 0.3970\n",
      "Epoch 58/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 609.2935 - loss: 45.8287 - reconstruction_loss: 0.3948\n",
      "Epoch 59/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - kl_loss: 578.0873 - loss: 43.7197 - reconstruction_loss: 0.3925\n",
      "Epoch 60/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 548.5957 - loss: 41.7190 - reconstruction_loss: 0.3898\n",
      "Epoch 61/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - kl_loss: 520.7231 - loss: 39.8209 - reconstruction_loss: 0.3873\n",
      "Epoch 62/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - kl_loss: 494.3749 - loss: 38.0198 - reconstruction_loss: 0.3845\n",
      "Epoch 63/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 469.4665 - loss: 36.3105 - reconstruction_loss: 0.3819\n",
      "Epoch 64/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 445.9160 - loss: 34.6882 - reconstruction_loss: 0.3794\n",
      "Epoch 65/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 423.6474 - loss: 33.1483 - reconstruction_loss: 0.3768\n",
      "Epoch 66/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 402.5879 - loss: 31.6860 - reconstruction_loss: 0.3744\n",
      "Epoch 67/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 382.6663 - loss: 30.2967 - reconstruction_loss: 0.3716\n",
      "Epoch 68/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 363.8228 - loss: 28.9772 - reconstruction_loss: 0.3689\n",
      "Epoch 69/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 346.0102 - loss: 27.7249 - reconstruction_loss: 0.3665\n",
      "Epoch 70/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - kl_loss: 329.1229 - loss: 26.5329 - reconstruction_loss: 0.3641\n",
      "Epoch 71/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - kl_loss: 313.1539 - loss: 25.4013 - reconstruction_loss: 0.3618\n",
      "Epoch 72/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 298.0355 - loss: 24.3250 - reconstruction_loss: 0.3597\n",
      "Epoch 73/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - kl_loss: 283.7222 - loss: 23.3018 - reconstruction_loss: 0.3571\n",
      "Epoch 74/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 270.1672 - loss: 22.3292 - reconstruction_loss: 0.3551\n",
      "Epoch 75/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 257.3274 - loss: 21.4030 - reconstruction_loss: 0.3527\n",
      "Epoch 76/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 245.1633 - loss: 20.5218 - reconstruction_loss: 0.3502\n",
      "Epoch 77/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 233.6393 - loss: 19.6840 - reconstruction_loss: 0.3480\n",
      "Epoch 78/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 222.7177 - loss: 18.8868 - reconstruction_loss: 0.3456\n",
      "Epoch 79/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 212.3650 - loss: 18.1278 - reconstruction_loss: 0.3435\n",
      "Epoch 80/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 202.5499 - loss: 17.4044 - reconstruction_loss: 0.3414\n",
      "Epoch 81/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 193.2408 - loss: 16.7148 - reconstruction_loss: 0.3390\n",
      "Epoch 82/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 184.4114 - loss: 16.0578 - reconstruction_loss: 0.3369\n",
      "Epoch 83/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 176.0351 - loss: 15.4317 - reconstruction_loss: 0.3348\n",
      "Epoch 84/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 168.0857 - loss: 14.8350 - reconstruction_loss: 0.3328\n",
      "Epoch 85/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 160.5402 - loss: 14.2659 - reconstruction_loss: 0.3305\n",
      "Epoch 86/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 153.3764 - loss: 13.7233 - reconstruction_loss: 0.3287\n",
      "Epoch 87/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 146.5848 - loss: 13.2061 - reconstruction_loss: 0.3266\n",
      "Epoch 88/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 140.1329 - loss: 12.7125 - reconstruction_loss: 0.3246\n",
      "Epoch 89/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 133.9996 - loss: 12.2414 - reconstruction_loss: 0.3227\n",
      "Epoch 90/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 128.1674 - loss: 11.7911 - reconstruction_loss: 0.3209\n",
      "Epoch 91/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 122.6200 - loss: 11.3614 - reconstruction_loss: 0.3190\n",
      "Epoch 92/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 117.3431 - loss: 10.9499 - reconstruction_loss: 0.3170\n",
      "Epoch 93/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 112.3209 - loss: 10.5572 - reconstruction_loss: 0.3154\n",
      "Epoch 94/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - kl_loss: 107.5393 - loss: 10.1815 - reconstruction_loss: 0.3138\n",
      "Epoch 95/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - kl_loss: 102.9990 - loss: 9.8295 - reconstruction_loss: 0.3121\n",
      "Epoch 96/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 98.6690 - loss: 9.4893 - reconstruction_loss: 0.3100 \n",
      "Epoch 97/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 94.5443 - loss: 9.1647 - reconstruction_loss: 0.3085\n",
      "Epoch 98/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 90.6148 - loss: 8.8545 - reconstruction_loss: 0.3070\n",
      "Epoch 99/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 86.8682 - loss: 8.5573 - reconstruction_loss: 0.3055\n",
      "Epoch 100/100\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - kl_loss: 83.2936 - loss: 8.2722 - reconstruction_loss: 0.3041\n",
      "\u001b[1m758/758\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 517us/step\n",
      "\u001b[1m758/758\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 419us/step\n",
      "Threshold used: 0.40875284075737023\n",
      "Predictions unique values: (array([0, 1]), array([22978,  1259]))\n",
      "type of y_pred: <class 'numpy.ndarray'>\n",
      "Precision: 0.00 ± 0.00\n",
      "Recall: 0.00 ± 0.00\n",
      "F1-Score: 0.00 ± 0.00\n"
     ]
    }
   ],
   "source": [
    "# Define pipeline\n",
    "vae_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', VAEClassifier(data_dim=transformed_sample.shape[1], normal_label=normal_label))\n",
    "])\n",
    "\n",
    "# Filter X_train and y_train based on normal label\n",
    "X_train_filtered = X_train[y_train == normal_label]\n",
    "y_train_filtered = y_train[y_train == normal_label]\n",
    "\n",
    "# Ensure X_train_filtered and y_train_filtered are converted to numpy arrays for proper indexing\n",
    "X_train_filtered = np.array(X_train_filtered)\n",
    "y_train_filtered = np.array(y_train_filtered)\n",
    "\n",
    "# Initialize lists to store metrics for each CV run\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Cross-validation with StratifiedKFold\n",
    "for i, (train_idx, test_idx) in enumerate(StratifiedKFold(n_splits=2).split(X_train_filtered, y_train_filtered)):\n",
    "    print(f\"[CV {i+1} ...]\")\n",
    "\n",
    "    # Split data\n",
    "    X_train_split, X_test_split = X_train_filtered[train_idx], X_train_filtered[test_idx]\n",
    "    y_train_split, y_test_split = y_train_filtered[train_idx], y_train_filtered[test_idx]\n",
    "\n",
    "    # Fit the pipeline on the normal samples from the training split\n",
    "    vae_pipeline.fit(X_train_split[y_train_split == normal_label], y_train_split[y_train_split == normal_label])\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = vae_pipeline.predict(X_test_split)\n",
    "\n",
    "    print(f\"type of y_pred: {type(y_pred)}\")\n",
    "    if not isinstance(y_pred, np.ndarray):\n",
    "        y_pred = np.array(y_pred)\n",
    "\n",
    "    # Ensure y_pred has the same length as y_test_split for metric calculation\n",
    "    if len(y_pred) != len(y_test_split):\n",
    "        raise ValueError(f\"Inconsistent prediction length: {len(y_pred)} vs {len(y_test_split)}\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision = precision_score(y_test_split, y_pred, pos_label=attack_label)  # Specify pos_label as needed\n",
    "    recall = recall_score(y_test_split, y_pred, pos_label=attack_label)\n",
    "    f1 = f1_score(y_test_split, y_pred, pos_label=attack_label)\n",
    "\n",
    "    # Append metrics for this CV run\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Calculate mean and standard deviation for each metric\n",
    "precision_mean = np.mean(precision_scores)\n",
    "precision_std = np.std(precision_scores)\n",
    "\n",
    "recall_mean = np.mean(recall_scores)\n",
    "recall_std = np.std(recall_scores)\n",
    "\n",
    "f1_mean = np.mean(f1_scores)\n",
    "f1_std = np.std(f1_scores)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Precision: {precision_mean:.2f} ± {precision_std:.2f}\")\n",
    "print(f\"Recall: {recall_mean:.2f} ± {recall_std:.2f}\")\n",
    "print(f\"F1-Score: {f1_mean:.2f} ± {f1_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
